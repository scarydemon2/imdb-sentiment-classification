{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNIKEY_COMP5046_Ass1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o0Tp2TB_Ywv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "34DVNKgqQY21"
      },
      "source": [
        "# 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7cWUxAQrGlq6"
      },
      "source": [
        "## 1.1. Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U7C4snIcNl22",
        "colab": {}
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1vF3FqgBC1Y-RPefeVmY8zetdZG1jmHzT'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('imdb_train.csv')\n",
        "\n",
        "id = '1XhaV8YMuQeSwozQww8PeyiWMJfia13G6'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('imdb_test.csv')\n",
        "\n",
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"imdb_train.csv\")\n",
        "df_test = pd.read_csv(\"imdb_test.csv\")\n",
        "\n",
        "reviews_train = df_train['review'].tolist()\n",
        "sentiments_train = df_train['sentiment'].tolist()\n",
        "reviews_test = df_test['review'].tolist()\n",
        "sentiments_test = df_test['sentiment'].tolist()\n",
        "\n",
        "print(\"Training set number:\",len(reviews_train))\n",
        "print(\"Testing set number:\",len(reviews_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l9gBSgBCQh24"
      },
      "source": [
        "## 1.2. Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "emyl1lWxGr12",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "#we do lower case the word then delete punctuation then delete html structe then tokenize then lemmatize,the preprocess is a pipeline concat these step\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def lower_case(x):#\n",
        "    x=x.lower()\n",
        "    return x\n",
        "\n",
        "def delete_punct(x):#delete punctuation such as .,:;\n",
        "    x = re.sub(r'[^\\w\\s]','',x)\n",
        "    return x\n",
        "\n",
        "def get_plain_text(x):#get text without html structure\n",
        "  x=BeautifulSoup(x).get_text()\n",
        "  return x\n",
        "\n",
        "def tokenize(x):#tokenize \n",
        "  x = word_tokenize(x)\n",
        "  return x\n",
        "\n",
        "def lemmatize(text,lemmatizer):#reductive tense\n",
        "  x = [lemmatizer.lemmatize(x, \"v\") for x in text]#lemmatize\n",
        "  return x\n",
        "\n",
        "def remove_stopword(text,stop_words):#\n",
        "  text = [x for x in text if x not in stop_words]#delete stopword\n",
        "  return text\n",
        "\n",
        "def preprocess(text,stop_words,lemmatizer):#connect the all preprocessing steps\n",
        "    text = get_plain_text(text)\n",
        "    text = lower_case(text)\n",
        "    text = delete_punct(text)\n",
        "    words= tokenize(text)\n",
        "    without_stopwords_words=remove_stopword(words,stop_words)\n",
        "    lemmatized=lemmatize(without_stopwords_words,lemmatizer)\n",
        "    \" \".join(lemmatized)\n",
        "    return lemmatized\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = stopwords.words(\"english\")\n",
        "\n",
        "print(\"starting processing \")\n",
        "reviews_train =[preprocess(review,stop_words,lemmatizer) for review in reviews_train]\n",
        "reviews_test =[preprocess(review,stop_words,lemmatizer) for review in reviews_test]\n",
        "print(\"successful\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlR0DPPwW0Vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(reviews_train[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LIu_lkJwQ55g"
      },
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "daDvAftceIvr"
      },
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3cM4rlYkHefJ",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "#not like BERT, we do not first pretrain a model, and fine tune this model. we train the model to our classification task directly.\n",
        "#so we do not use word2vec .the word2vec is more like BERT.that extract infomation from plain without classifier task.\n",
        "# we use fasttext ,a simple linear model. we use the nn.Embedding.data as our final word embedding vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GXgFpxIgl-_G"
      },
      "source": [
        "### 2.1.1. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3LByzHLiNinu",
        "colab": {}
      },
      "source": [
        "#we first get the all word(the frequency is larger than 10) from all reviews,note that the index of word start from 0,and 0 is padding token ,the common word index is from 1\n",
        "#and we convert all review to its word ids,if a word is not in word list ,the id will be 0 (pad),if a single is not long enough or less than the average length of reviews .padding will fill the last id\n",
        "\n",
        "\n",
        "from collections import Counter#a counter uesd for get the occure times of a word\n",
        "\n",
        "def get_average_length(all_reviews):#calculate the average length of the all reviews\n",
        "  average_review_length=0\n",
        "  for review in all_reviews:\n",
        "    average_review_length+=len(review)\n",
        "  return average_review_length//len(all_reviews)\n",
        "\n",
        "def binarySentiment(sentiment):#pos-->1 ,neg-->0\n",
        "    return [1 if sent=='pos' else 0 for sent in sentiment]\n",
        "\n",
        "def reviews_to_ids(word2id,data):#convert the reviews to its word id list\n",
        "  reviews_ids=[]\n",
        "  for review in data:\n",
        "    review_ids=[]\n",
        "    review_ids=[word2id.get(word,0) for word in review]#if the word do not exist ,use 0 instead ,else use the word id\n",
        "    review_ids=review_ids[:average_reviews_length]\n",
        "    if len(review)<average_reviews_length:#padding \n",
        "      padding_length=average_reviews_length-len(review_ids)\n",
        "      review_ids=review_ids+[0]*padding_length\n",
        "    reviews_ids.append(review_ids)\n",
        "  return reviews_ids\n",
        "\n",
        "all_reviews=reviews_train+reviews_test#\n",
        "all_words=['<pad>']#the first word is padding token the index is 0\n",
        "for review in all_reviews:\n",
        "  for word in review:\n",
        "    all_words.append(word)\n",
        "counter=Counter(word for word in all_words)\n",
        "id2word=[word for word,frequency in counter.items() if frequency >=10]#a list convert wordid to word \n",
        "id2word.sort()\n",
        "word2id={ w : i for i,w in enumerate(id2word)}#dict convert word to its id\n",
        "\n",
        "average_reviews_length=get_average_length(all_reviews)#119\n",
        "average_reviews_length=128   #average length is 119, we use 128 the nearest power of 2\n",
        "print('average review length is ',average_reviews_length)\n",
        "\n",
        "sentiment_train=binarySentiment(sentiments_train)#pos to 1,neg to 0\n",
        "sentiment_test=binarySentiment(sentiments_test)\n",
        "\n",
        "reviews_ids = reviews_to_ids(word2id,reviews_train)#convert all train reviews\n",
        "voc_size = len(id2word)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olsCeHXLPaLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(reviews_ids[0])\n",
        "print(sentiment_train)\n",
        "print(voc_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qhAgWf_AmbZ8"
      },
      "source": [
        "### 2.1.2. Build Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TVPuwWgvNjOU",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "\n",
        "input_dim=128\n",
        "embedding_size=200 #we get 200dim fasttext embedding\n",
        "output_dim=2#because it's a binary classification\n",
        "num_of_epochs = 20\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "\n",
        "\n",
        "class fasttext(nn.Module):\n",
        "    def __init__(self,embedding_size,output_dim):\n",
        "        super(fasttext, self).__init__()\n",
        "        self.embedding = nn.Embedding(voc_size,embedding_size)\n",
        "        self.projection=nn.Linear(embedding_size,embedding_size)#many paper use such projection,so I use it .lol\n",
        "        self.linear1 = nn.Linear(embedding_size,32,bias=True)#two simple linear layer \n",
        "        self.linear2 = nn.Linear(32,output_dim,bias=False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embedding(inputs)\n",
        "        #50*128*200\n",
        "        embeds=self.projection(embeds)#50*128*200\n",
        "        embed_mean = torch.mean(embeds,dim=1)#because we should use a 2dim tensor not 3D\n",
        "        # print(embed_mean.size)\n",
        "        out = self.linear1(embed_mean)\n",
        "        out = self.linear2(out)\n",
        "        return out\n",
        "\n",
        "train_input=reviews_to_ids(word2id,reviews_train)#load train input and test input \n",
        "\n",
        "train_input=reviews_to_ids(word2id,reviews_train)#load train input and test input \n",
        "train_input=torch.tensor(train_input).long().to(device)#convert the numpy to tensor\n",
        "\n",
        "print(train_input.size())\n",
        "\n",
        "train_label=torch.tensor(sentiment_train).to(device)#load train target and test target and convert to tensor\n",
        "print(train_label.size())\n",
        "\n",
        "#generate batch from Dataloader,and TensorDataset\n",
        "train_input_dataset=TensorDataset(train_input,train_label)#create dataset\n",
        "train_input_batch=DataLoader(train_input_dataset,batch_size=100,shuffle=False)#create dataloader from dataset\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LNys5HOdISK-"
      },
      "source": [
        "### 2.1.3. Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ae8i7Z2kIef-",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "\n",
        "model = fasttext(embedding_size,output_dim).to(device)#set model\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)#set optimizer ,we use Adam,\n",
        "\n",
        "for epoch in range(num_of_epochs):\n",
        "    epoch_loss = 0#total loss in one epoch\n",
        "    correct = 0#use to calculate acc\n",
        "    total_num = 0\n",
        "    for step,batch in enumerate(train_input_batch):\n",
        "        batch_in_torch,batch_tar_torch=batch\n",
        "        model.train()#set to the train mode\n",
        "        optimizer.zero_grad()#delete the grad that already consist in optimizer\n",
        "        outputs = model(batch_in_torch)#get output\n",
        "        print(outputs.size())\n",
        "        # print(\"output size\",outputs.size())\n",
        "        loss = criterion(outputs, batch_tar_torch)#get loss\n",
        "        loss.backward()\n",
        "        optimizer.step()#SGD\n",
        "        epoch_loss += loss.item()\n",
        "        correct += torch.sum(outputs.argmax(dim=1)==batch_tar_torch).item()\n",
        "        total_num += len(labels)\n",
        "        print(\"\\r Epoch:%d, loss:%s, acc:%s\"%(epoch+1,epoch_loss,correct*1.0/total_num),end=\"\")\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uMCv3YI1IfUo"
      },
      "source": [
        "### 2.1.4. Save Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3OwicNPkIqd1",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "\n",
        "fasttest_embedding = model.embedding.weight.data#get the embedding\n",
        "trained_embeddings = fasttest_embedding.detach().cpu().numpy()#\n",
        "\n",
        "torch.save(trained_embeddings,\"./fasttext_embedding.pkl\")#save the embedding to local\n",
        "torch.save(word2id,\"./word2id.pkl\")#and the corresponding word \n",
        "torch.save(model,'fasttext_model.pt')#save the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5u52vY1zIon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(trained_embeddings.shape)#vocab num.,embedding dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yn16xrDrIs8B"
      },
      "source": [
        "### 2.1.5. Load Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-IebpYFsIvgh",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "\n",
        "fasttext_embedding = torch.load('./fasttext_embedding.pkl')#load the embedding\n",
        "word2id = torch.load('./word2id.pkl')#and the word\n",
        "print(len(fasttext_embedding),len(fasttext_embedding[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T0ap96aeGlIk"
      },
      "source": [
        "## 2.2. Character Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d16v3oKaGlI0"
      },
      "source": [
        "### 2.2.1. Data Preprocessing for Character Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i2CUCL1cGlI2",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "# To get char embedding,we need a character2id table.The character should be lower case,beacuse of we have changed the review to lower case. for every word\n",
        "# we will change it to a tensor .for example,the word \"ab\" will change to [[1,0,0,0,0,...,0],[0,1,0,0...0]] AKA onehot code.The shape of a word should be (word_length,1,37) ,37=26character +padding+0-9number\n",
        "\n",
        "char_set=set()#get all character ever occured in word\n",
        "for word in id2word:\n",
        "  char_set.update(word.lower())#get all character\n",
        "\n",
        "id2char=['*']+list(char_set)#\"* is the padding symbol\n",
        "print(id2char)\n",
        "char2id={c:i for i,c in enumerate(id2char)}\n",
        "dic_len=len(char2id)\n",
        "print('char table have',dic_len)\n",
        "\n",
        "\n",
        "seq_data = id2word#char embedding only for the word that embedded by word2vec,If a word is un-embedded ,we can using zero to init its embedding\n",
        "padded_seq_data=[]#save the all padded words\n",
        "average_length=0\n",
        "for word in id2word:\n",
        "    average_length+=len(word)\n",
        "average_length=average_length//len(id2word)\n",
        "print(\"average word length is \",average_length)#7\n",
        "\n",
        "\n",
        "for word in seq_data:#padding process\n",
        "  if len(word)>=average_length+1:#because I use // not /,when compute average_length\n",
        "    padded_seq_data.append(word[:average_length+1])\n",
        "  else:\n",
        "    pad='*'*(average_length+1-len(word))\n",
        "    pad+=word\n",
        "    padded_seq_data.append(pad)#average word length\n",
        "print(padded_seq_data)\n",
        "\n",
        "\n",
        "def make_batch(seq_data):#same as lab\n",
        "  input_batch = []\n",
        "  target_batch = []\n",
        "  i=0\n",
        "  for seq in seq_data:\n",
        "    i+=1\n",
        "    input_data = [char2id[n] for n in seq[:-1]]\n",
        "    padd_num=0\n",
        "    for i in input_data:\n",
        "      if i==26:\n",
        "        padd_num+=1\n",
        "\n",
        "    target = char2id[seq[-1]]\n",
        "    single_one_hot=np.eye(dic_len)[input_data]\n",
        "    single_one_hot[:padd_num]=[0]\n",
        "    input_batch.append(single_one_hot)\n",
        "    target_batch.append([target])\n",
        "\n",
        "  return input_batch, target_batch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zgiOPcsTGlI6"
      },
      "source": [
        "### 2.2.2. Build Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jj3YZ3PWGlI8",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "#we choose Bi-LSTM which is strength in many task.Such a toy job is not worth to use a deep network .\n",
        "#and the hidden dimsion is 200(bidirection * 100)，This is a classification task ,we can use crossEntropy that is useful in\n",
        "#classification task.We use Adam to optim our training .\n",
        "\n",
        "learning_rate=0.001\n",
        "\n",
        "n_hidden=100\n",
        "# number of inputs (dimension of input vector )\n",
        "n_input = dic_len\n",
        "# number of classes = 37\n",
        "n_class = dic_len\n",
        "class Net(nn.Module):#一个简单的LSTM分类器\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.lstm = nn.LSTM(n_input, n_hidden, batch_first=True, bidirectional=True, dropout=0.2)\n",
        "        self.linear = nn.Linear(n_hidden * 2, n_class)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        # h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n",
        "        lstm_out, (h_n, c_n) = self.lstm(sentence)#h_n也就是hidden，隐藏着上下文信息\n",
        "        # concat the last hidden state from two direction\n",
        "        hidden_out = torch.cat((h_n[0, :, :], h_n[1, :, :]), 1)#我们把双向的lstm得到的hidden合并成一个\n",
        "        z = self.linear(hidden_out)#一次projection\n",
        "        log_output = F.log_softmax(z, dim=1)#得到结果，用于计算loss\n",
        "        return log_output, F.softmax(hidden_out, dim=1)\n",
        "\n",
        "\n",
        "# Move the model to GPU\n",
        "net = Net().to(device)\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()#交叉熵损失函数\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)#Adam优化器\n",
        "\n",
        "# Preparing input\n",
        "input_batch, target_batch = make_batch(padded_seq_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "46W0zFfWGlI_"
      },
      "source": [
        "### 2.1.4. Train Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UWQn-VyNGlJA",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "learning_rate = 0.001\n",
        "n_hidden = 100\n",
        "total_epoch = 50\n",
        "from torch.utils.data import DataLoader, TensorDataset#使用Dataloader来分batch\n",
        "\n",
        "# Convert input into tensors and move them to GPU by uting tensor.to(device)\n",
        "input_batch_torch = torch.tensor(input_batch).float().to(device)#convert batch to tensor \n",
        "target_batch_torch = torch.tensor(target_batch).view(-1).to(device)\n",
        "\n",
        "train_data=TensorDataset(input_batch_torch,target_batch_torch)#use dataloader to generate batch\n",
        "train_dataloader=DataLoader(train_data,batch_size=50)#50一个batch\n",
        "train_batches = [batch for batch in train_dataloader]\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "    for step,batch in enumerate(train_batches):\n",
        "        # Set the flag to training\n",
        "        net.train()#set to train mode\n",
        "        input_mini_batch_torch,target_mini_batch_torch=batch#one batch\n",
        "        input_mini_batch_torch.to(device)\n",
        "        target_mini_batch_torch.to(device)\n",
        "        # forward + backward + optimize\n",
        "        outputs,_ = net(input_mini_batch_torch)#get output\n",
        "        loss = criterion(outputs, target_mini_batch_torch)#calculate loss\n",
        "        loss.backward()#\n",
        "        optimizer.step()#SGD\n",
        "        optimizer.zero_grad()#delete the grad that contain in the optimizer\n",
        "\n",
        "        # Set the flag to evaluation, which will 'turn off' the dropout\n",
        "        net.eval()#set to the eval mode.\n",
        "        outputs,_= net(input_mini_batch_torch)#and get the output \n",
        "\n",
        "        # Evaluation loss and accuracy calculation\n",
        "        loss = criterion(outputs, target_mini_batch_torch)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        acc = accuracy_score(predicted.cpu().numpy(), target_mini_batch_torch.cpu().numpy())\n",
        "\n",
        "        print('Epoch: %d, loss: %.5f, train_acc: %.2f' % (epoch + 1, loss.item(), acc))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "#get char embedding\n",
        "net.eval()\n",
        "_,hidden_state = net(input_batch_torch)#the hidden state contain thte context info, so we use hidden state of rnn as the cahr embedding\n",
        "hidden_state=hidden_state.cpu().data.numpy()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R5Bym9bBGlJE"
      },
      "source": [
        "### 2.1.5. Save Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ggTsYIm7GlJF",
        "colab": {}
      },
      "source": [
        "\n",
        "def save_embedding(hidden_state,id2word, file_name):#save the embedding to the local\n",
        "    embedding = hidden_state#(word number,100)\n",
        "    fout = open(file_name, 'w')\n",
        "    fout.write('%d %d\\n' % (len(id2word), len(embedding[0])))\n",
        "    for wid, w in enumerate(id2word):\n",
        "        e = embedding[wid]\n",
        "        e = ' '.join(map(lambda x: str(x), e))\n",
        "        fout.write('%s %s\\n' % (w, e))\n",
        "save_embedding(hidden_state,id2word,'./char2vec_embedding.txt')\n",
        "torch.save(net,'./char2vec_model.pt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JwOI-wIKGlJI"
      },
      "source": [
        "### 2.1.6. Load Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4-jyj-lOHWWj",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "char2vec_model=torch.load('./char2vec_model.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tlCeWT8eeLnd"
      },
      "source": [
        "## 2.3. Sequence model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fwA-NN3EJ4Ig"
      },
      "source": [
        "### 2.3.1. Apply/Import Word Embedding and Character Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g7PKX1gIePA2",
        "colab": {}
      },
      "source": [
        "#\n",
        "\n",
        "#load fasttext embedding \n",
        "fasttext_embedding = torch.load('./fasttext_embedding.pkl')\n",
        "fasttext_embedding=torch.tensor(fasttext_embedding).float()\n",
        "word2id = torch.load('./word2id.pkl')\n",
        "\n",
        "#load char2vec emebdding by hand from char2vec_embedding.txt\n",
        "def build_embedding(id2word, file, dim):#dim is 200 .that is because rnn is bidirectional so 100*2=200\n",
        "    vocab_size = len(id2word)\n",
        "    emb = np.random.uniform(-1, 1, (vocab_size, dim))\n",
        "    emb[0] = 0 # <pad> should be all 0\n",
        "    flag = 0\n",
        "    w2id = {w: i for i, w in enumerate(id2word)}#id to char embedding\n",
        "    with open(file, encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            if flag==0:\n",
        "                flag+=1\n",
        "                continue\n",
        "            elems = line.split()\n",
        "            token = ''.join(elems[0:-dim])#\n",
        "            if token in w2id:#if word in the w2id\n",
        "                emb[w2id[token]] = [float(v) for v in elems[-dim:]]#then get the embedding\n",
        "    return emb\n",
        "\n",
        "char2vec_embedding=build_embedding(id2word,'./char2vec_embedding.txt',200)\n",
        "char2vec_embedding=torch.tensor(char2vec_embedding).float()\n",
        "print(char2vec_embedding.size())\n",
        "\n",
        "#concat two embedding ,get a 400 dim embedding\n",
        "embedding=torch.cat((char2vec_embedding,fasttext_embedding),dim=1)\n",
        "print(embedding.size())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DpYCL17JKZxl"
      },
      "source": [
        "### 2.3.2. Build Sequence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R204UIyDKhZ4"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "13eCtR_SLUG6",
        "colab": {}
      },
      "source": [
        "# the model use the pretrained fast text embedding +char embedding, so the input dim is fixed to 400,drop out rate 0.2 is useful\n",
        "#and then 400dim will reduce to 32 then 2(for pos or neg)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
        "\n",
        "class imdbClassifier(nn.Module):\n",
        "    def __init__(self,embedding,n_hidden = 128):\n",
        "        super(imdbClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.Tensor(embedding))\n",
        "        self.drop_out = nn.Dropout(0.2)\n",
        "        n_input = embedding.shape[1]#400 200+200from pretrained embedding\n",
        "        self.lstm = nn.LSTM(n_input, n_hidden, num_layers=2, batch_first =True,bidirectional=True)\n",
        "        self.linear = nn.Linear(n_hidden*2 ,32)\n",
        "        self.linear2 = nn.Linear(32 ,2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "#         print(x.shape)\n",
        "        x,_ = self.lstm(x)\n",
        "        x = self.drop_out(x[:,-1,:])\n",
        "        x = self.linear(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "imdb = imdbClassifier(embedding).to(device)\n",
        "print(imdb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6BaOiaGRLW7R"
      },
      "source": [
        "### 2.3.3. Train Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lVQnUSX1LZ6C",
        "colab": {}
      },
      "source": [
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset#使用Dataloader来分batch\n",
        "\n",
        "\n",
        "train_input=reviews_to_ids(word2id,reviews_train)#load train input and test input \n",
        "test_input=reviews_to_ids(word2id,reviews_test)\n",
        "train_input=torch.tensor(train_input).long().to(device)#convert the numpy to tensor\n",
        "test_input=torch.tensor(test_input).long().to(device)\n",
        "print(train_input.size())\n",
        "\n",
        "\n",
        "train_label=torch.tensor(sentiment_train).to(device)#load train target and test target and convert to tensor\n",
        "test_label=torch.tensor(sentiment_test).to(device)#\n",
        "print(train_label.size())\n",
        "\n",
        "#generate batch from Dataloader,and TensorDataset\n",
        "train_input_dataset=TensorDataset(train_input,train_label)#create dataset\n",
        "train_input_batch=DataLoader(train_input_dataset,batch_size=100,shuffle=False)#create dataloader from dataset\n",
        "test_input_dataset=TensorDataset(test_input,test_label)\n",
        "test_input_batch=DataLoader(test_input_dataset,batch_size=100,shuffle=False)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.0004# \n",
        "optimizer = optim.Adam(imdb.parameters(), lr=learning_rate)\n",
        "batch_size=100\n",
        "total_epoch=25#if set to 30 ,will over fit\n",
        "\n",
        "best_state_dict = None\n",
        "best_f1 = 0 #log the best f1\n",
        "\n",
        "\n",
        "for epoch in range(total_epoch):     \n",
        "    epoch_loss = 0#total loss of a epoch\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    imdb.train()\n",
        "    for step,batch in enumerate(train_input_batch):#get a batch data\n",
        "        optimizer.zero_grad()#delete the grad that conatin in the optimizer\n",
        "        input_batch_torch,target_batch_torch=batch\n",
        "        outputs = imdb(input_batch_torch)#get outputs\n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        predicted = torch.argmax(outputs,dim=1)#predicted is a 0/1 list,\n",
        "        correct += torch.sum(predicted==target_batch_torch)#current step correct number \n",
        "        total += batch_size#total correct number\n",
        "        loss.backward()#SGD\n",
        "        optimizer.step()        \n",
        "        epoch_loss+=loss.item()#total loss in this epoch\n",
        "        acc = (correct*1.0/total).item()\n",
        "        print('\\rEpoch: %d, loss: %.2f, acc:%s' %(epoch + 1,epoch_loss, acc),end=\"\")\n",
        "    print() \n",
        "    imdb.eval()\n",
        "    predicted_labels = []#do eval\n",
        "    true_labels = []\n",
        "    with torch.no_grad():\n",
        "      for batch in test_input_batch:\n",
        "          input_batch,target_batch=batch\n",
        "          input_batch_torch = torch.from_numpy(np.array(input_batch.cpu())).long().to(device)\n",
        "          outputs = imdb(input_batch_torch) \n",
        "          predicted = torch.argmax(outputs,dim=1).cpu().numpy().tolist()\n",
        "          labels = target_batch.cpu().data\n",
        "\n",
        "      acc = accuracy_score(predicted,labels)\n",
        "      recall = recall_score(predicted,labels)\n",
        "      current_f1 = f1_score(predicted,labels)\n",
        "      log.append((epoch,current_f1))#use for the finall graph\n",
        "      if current_f1 > best_f1:#we only save the best model \n",
        "          torch.save(imdb.state_dict(),\"temp_imdb.pkl\")\n",
        "          best_state_dict = torch.load(\"temp_imdb.pkl\")\n",
        "          best_f1 = current_f1\n",
        "      \n",
        "    print(\"epoch: %s, test f1: %s\"%(epoch+1,current_f1))\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-2feNpG-LZx2"
      },
      "source": [
        "### 2.3.4. Save Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sflUAgV4L1o8",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "torch.save(best_state_dict,\"./imdb.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4zFo6YppL6w3"
      },
      "source": [
        "### 2.3.5. Load Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OtNxLzDGMCan",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "\n",
        "imdb = imdbClassifier(embedding).to(device)\n",
        "imdb.load_state_dict(torch.load(\"./imdb.pkl\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a4mpRpocePLN"
      },
      "source": [
        "# 3 - Evaluation\n",
        "\n",
        "(*Please show your empirical evidence*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KEW1zMgVMREr"
      },
      "source": [
        "## 3.1. Performance Evaluation\n",
        "\n",
        "\n",
        "You are required to provide the table with precision, recall, f1 of test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LPHCb-bneTI9",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "imdb.eval()\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "test_input_eval=torch.tensor(test_input[:1000]).long()#only use 1000 data to eval ,otherwise the GPU will crash\n",
        "print(test_input_eval.size())\n",
        "test_label=torch.tensor(sentiment_test[:1000])\n",
        "print(test_label.size())\n",
        "with torch.no_grad():#same as above\n",
        "        outputs = imdb(test_input_eval) \n",
        "        predicted = torch.argmax(outputs,dim=1).cpu().numpy().tolist()\n",
        "        labels = target_batch\n",
        "        true_labels.extend(labels)\n",
        "acc = accuracy_score(predicted,test_label.cpu())\n",
        "recall = recall_score(predicted,test_label.cpu())\n",
        "f1 = f1_score(predicted,test_label.cpu())\n",
        "print(\"precision:%s, recall:%s, f1:%s\"%(acc,recall,f1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P28Z1k36MZuo"
      },
      "source": [
        "## 3.2. Hyperparameter Testing\n",
        "*You are required to draw a graph(y-axis: f1, x-axis: epoch) for test set and explain the optimal number of epochs based on the learning rate you have already chosen.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wTLyQEeZMZ2f",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "#as is shown in the graph the best model is when epoch between 10-15, the f1 is 0.9 .very good!!\n",
        "import matplotlib.pyplot as plt\n",
        "epochs,f1 = zip(*log)\n",
        "plt.title(\" Hyperparameter Testing  lol\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"f1\")\n",
        "plt.plot(epochs,f1)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sfv8rWTKPzeb"
      },
      "source": [
        "## Object Oriented Programming codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TS23AjBRSZaX"
      },
      "source": [
        "*You can use multiple code snippets. Just add more if needed* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1hVmx4E52dXS",
        "colab": {}
      },
      "source": [
        "# If you used OOP style, use this section"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}